{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Calculate word embedings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "htCkSTTcsJXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk                                # Python library for NLP\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "from sklearn.decomposition import PCA      # PCA library"
      ],
      "metadata": {
        "id": "5YAad9hcuVFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/train_token.csv', encoding='latin-1')\n",
        "test = pd.read_csv('/content/drive/MyDrive/test_token.csv', encoding='latin-1')"
      ],
      "metadata": {
        "id": "Trb7uI6Bukvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[~train.iloc[:]['TweetTokens'].isnull()]\n",
        "test = test[~test.iloc[:]['TweetTokens'].isnull()]"
      ],
      "metadata": {
        "id": "gvD3iemJ05g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_dictionary = pd.read_csv('/content/drive/MyDrive/words_dictionary.csv', encoding='latin-1')\n",
        "words_dictionary = words_dictionary[~words_dictionary['Unnamed: 0'].isnull()]\n",
        "vocabulary = set(words_dictionary['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "iFF5MRXr_ziD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_words(words, vocabulary):\n",
        "  '''\n",
        "  Input:\n",
        "     words = list of strings/words\n",
        "     vocabulary = list of strings/words\n",
        "\n",
        "  Output:\n",
        "     new_list = list of words from input words that are in vocabulary\n",
        "  '''\n",
        "  new_list = []\n",
        "  for word in words:\n",
        "    if word in vocabulary:\n",
        "      new_list.append(word)\n",
        "\n",
        "  return new_list"
      ],
      "metadata": {
        "id": "jGbZaE8744du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sentences, list of lists of words of tweets "
      ],
      "metadata": {
        "id": "mp8WTwLTbnDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = []\n",
        "\n",
        "for i in range(0,len(train)):\n",
        "  list_of_words = re.sub(\"[^\\w]\", \" \", train.iloc[i]['TweetTokens']).split()\n",
        "  list_of_words = remove_words(list_of_words, vocabulary)\n",
        "  sentence.append(list_of_words)"
      ],
      "metadata": {
        "id": "7F9IQZ7ruwkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "SIZE = 100\n",
        "model = Word2Vec(sentence, min_count=1, size=SIZE)\n",
        "embedings = model.wv"
      ],
      "metadata": {
        "id": "SfPkmeYJsPK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embedings['covid']"
      ],
      "metadata": {
        "id": "KOLxpH5btkYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_matrices(set_of_words, embedings):\n",
        "    \"\"\"\n",
        "    Output:\n",
        "        X: English words to their corresponding word embeddings. \n",
        "        Y: a matrix where the columns correspong to the embeddings.\n",
        "    \"\"\"\n",
        "    Y_l = list()\n",
        "    X = []\n",
        "\n",
        "    for word in set_of_words:\n",
        "      Y_l.append(embedings[word])\n",
        "      X.append(word)\n",
        "\n",
        "    # stack the vectors of Y_l into a matrix Y\n",
        "    Y = np.stack(Y_l)\n",
        "    return X,Y"
      ],
      "metadata": {
        "id": "GfbpZlkF1vCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_words, coordinates = get_matrices(vocabulary, embedings)"
      ],
      "metadata": {
        "id": "bAWReW4h68U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pca(X, n_components: int=2):\n",
        "    \"\"\"Calculate the principal components for X\n",
        "\n",
        "    Args:\n",
        "       X: of dimension (m,n) where each row corresponds to a word vector\n",
        "       n_components: Number of components you want to keep.\n",
        "\n",
        "    Return:\n",
        "       X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
        "    \"\"\"\n",
        "    # you need to set axis to 0 or it will calculate the mean of the entire matrix instead of one per row\n",
        "    X_demeaned = X - X.mean(axis=0)\n",
        "\n",
        "    # calculate the covariance matrix\n",
        "    # the default numpy.cov assumes the rows are variables, not columns so set rowvar to False\n",
        "    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n",
        "\n",
        "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
        "    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix)\n",
        "\n",
        "    # sort eigenvalue in increasing order (get the indices from the sort)\n",
        "    idx_sorted = np.argsort(eigen_vals)\n",
        "\n",
        "    # reverse the order so that it's from highest to lowest.\n",
        "    idx_sorted_decreasing = list(reversed(idx_sorted))\n",
        "\n",
        "    # sort the eigen values by idx_sorted_decreasing\n",
        "    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n",
        "\n",
        "    # sort eigenvectors using the idx_sorted_decreasing indices\n",
        "    # We're only sorting the columns so remember to get all the rows in the slice\n",
        "    eigen_vecs_sorted = eigen_vecs[:, idx_sorted_decreasing]\n",
        "\n",
        "    # select the first n eigenvectors (n is desired dimension\n",
        "    # of rescaled data array, or dims_rescaled_data)\n",
        "    # once again, make sure to get all the rows and only slice the columns\n",
        "    eigen_vecs_subset = eigen_vecs_sorted[:, :n_components]\n",
        "\n",
        "    # transform the data by multiplying the transpose of the eigenvectors \n",
        "    # with the transpose of the de-meaned data\n",
        "    # Then take the transpose of that product.\n",
        "    X_reduced = np.dot(eigen_vecs_subset.T, X_demeaned.T).T\n",
        "    return X_reduced"
      ],
      "metadata": {
        "id": "J-7NqJoy7oG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_coordinates = compute_pca(coordinates)"
      ],
      "metadata": {
        "id": "QqlvO4Ro7wiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fig, ax = plt.subplots()\n",
        "#plt.xlim([0.25,0.45])\n",
        "#plt.ylim([-0.05,0.05])\n",
        "\n",
        "#for i in range(10,15):\n",
        "#  ax.scatter(reduced_coordinates[i,0], reduced_coordinates[i,1])\n",
        "#  ax.annotate(list_of_words[i], (reduced_coordinates[i,0]-0.001, reduced_coordinates[i,1]-0.001))\n",
        "\n",
        "#plt.title('Primjer pozicije rijeƒçi nakon primjene LSH i PCA')\n",
        "#plt.xlabel('prva koordinata')\n",
        "#plt.ylabel('druga koordinata')\n"
      ],
      "metadata": {
        "id": "x_S5FwOn83i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_document_embedding(tweet, embeddings):\n",
        "    '''\n",
        "    Input:\n",
        "        - tweet: tweet tokens\n",
        "        - embeddings: a dictionary of word embeddings\n",
        "    Output:\n",
        "        - doc_embedding: sum of all word embeddings in the tweet\n",
        "    '''\n",
        "    doc_embedding = np.zeros(SIZE)\n",
        "    for word in tweet:\n",
        "      doc_embedding = doc_embedding + embedings[word] \n",
        "\n",
        "    return doc_embedding"
      ],
      "metadata": {
        "id": "1vYGnn2G95ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tweets = np.zeros((len(train),SIZE))\n",
        "\n",
        "for i in range(0,len(train)):\n",
        "  list_of_words = re.sub(\"[^\\w]\", \" \", train.iloc[i]['TweetTokens']).split()\n",
        "  list_of_words = remove_words(list_of_words, vocabulary)\n",
        "  train_tweets[i,:] = get_document_embedding(list_of_words, embedings)"
      ],
      "metadata": {
        "id": "y_XNSaqs_IBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(train_tweets).to_csv('/content/drive/MyDrive/train_lsh.csv',index = False)"
      ],
      "metadata": {
        "id": "TE8HeKibKbEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = np.zeros((len(test),SIZE))\n",
        "\n",
        "for i in range(0,len(test)):\n",
        "  list_of_words = re.sub(\"[^\\w]\", \" \", test.iloc[i]['TweetTokens']).split()\n",
        "  list_of_words = remove_words(list_of_words, vocabulary)\n",
        "  test_tweets[i,:] = get_document_embedding(list_of_words, embedings)"
      ],
      "metadata": {
        "id": "GGGITyPaBu90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(test_tweets).to_csv('/content/drive/MyDrive/test_lsh.csv',index = False)"
      ],
      "metadata": {
        "id": "thM_lyzRBusL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}